# [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)

Language models (LMs) are challenging to adapt to new distributions by simply finetuning because their subword tokenizers remain unchanged during adaptation. FLEXITOKENS addresses this by using a simplified training objective that enables significantly greater flexibility during adaptation.



![FLEXITOKENS](paper/flexitoken_vs_BPE.png)

*An example of tokenized medical text, where FLEXITOKENS produces a less fragmented sequence of tokens than BPE. Unlike BPE which applies a fixed tokenization, FLEXITOKENS adapts its tokenization to the medical domain, capturing domain-specific patterns more effectively.*

## ğŸ“ Repository Structure

```
flexitokens/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .gitignore               # Git ignore rules
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ model/              # Model implementations
â”‚   â”œâ”€â”€ train/              # Training scripts and utilities
â”‚   â”œâ”€â”€ eval/               # Evaluation scripts
â”‚   â”œâ”€â”€ finetune/           # Finetuning utilities
â”‚   â””â”€â”€ utils/              # Common utilities
â”‚
â”œâ”€â”€ configs/                 # Configuration files
â”‚   â”œâ”€â”€ train/              # Training configurations
â”‚   â”œâ”€â”€ finetune/           # Finetuning configurations
â”‚   â””â”€â”€ accelerate/         # Accelerate configurations
â”‚
â”œâ”€â”€ scripts/                 # Execution scripts
â”‚   â”œâ”€â”€ run_train.sh        # Main training script
â”‚   â”œâ”€â”€ eval/               # Evaluation scripts
â”‚   â”‚   â””â”€â”€ eval_pretrain.sh
â”‚   â””â”€â”€ finetune/           # Finetuning scripts
â”‚       â”œâ”€â”€ sib200_routing.sh
â”‚       â””â”€â”€ ner_wikiann.sh
â”‚
â”œâ”€â”€ data/                    # Dataset directory (created after download)
â”œâ”€â”€ model_ckpts/            # Model checkpoints
â”œâ”€â”€ results/                # Experimental results
â””â”€â”€ paper/                  # Paper and documentation
```

## Environment Setup

### Installation
```bash
# Create conda environment
conda create -n fxt python=3.8
conda activate fxt

# Install dependencies
pip install -r requirements.txt
```

## Data

We use multilingual data sampled from Fineweb and FineWeb2 Our training data includes multiple languages with different scripts:

- **Languages**: English (en), Spanish (es), Russian (ru), Ukrainian (uk), Hindi (hi), Telugu (te)
- **Scripts**: Latin, Cyrillic, Devanagari, Telugu

### Data Setup
- Data will be automatically downloaded from HuggingFace on first run
- Set `load_from_disk: false` in config for initial download
- Set `load_from_disk: true` for subsequent runs to use downloaded data

## Configuration

Configuration files are located in `configs/`. Key sections to modify:

### Training Config (`configs/train/`)
- **`boundaries`**: Script-specific tokenization settings
  - `prior_list`: Tokenization priors controlling compression rates
- **`data`**: Dataset paths and language settings
- **`model`**: Model architecture parameters


### Pretraining

1. **Select Configuration**: Choose or create a config file in `configs/train/`
   ```bash
   # Example configs available:
   ls configs/train/
   ```

2. **Set Directories**: Update paths in your config:
   - `data`: Path to your data directory
   - `cache_dir`: Path for caching processed datasets
   - Experiment output directory

3. **First Run Setup**: 
   - Set `load_from_disk: false` to download datasets from HuggingFace
   - This downloads the exact dataset we used to your data directory

4. **Subsequent Runs**:
   - Set `load_from_disk: true` to use the downloaded data after initial download

```bash
# Run pretraining with FlexiTokens
bash scripts/run_train.sh
```

### Evaluation
```bash
# Evaluate pretrained model
bash scripts/eval/eval_pretrain.sh
```

### Finetuning
```bash
# SIB-200 multilingual benchmark
bash scripts/finetune/sib200_routing.sh
```

## ğŸ“ Citation
If you use FlexiTokens in your research, please cite our paper:
```bibtex
@article{owodunniflexitokens,
  title={FlexiTokens: Flexible Tokenization for Evolving Language Models},
  author={Owodunni, Abraham Toluwase and Ahia, Orevaoghene and Kumar, Sachin}
}
```


